\documentclass{article}
\usepackage{../../style/header}

\begin{document}

\title{Algebra 3}
\author{Lectured by Alession Corti \\
Scribed by Yu Coughlin}
\date{Autumn 2025}

\maketitle

\tableofcontents

\section{Rings and modules}

\subsection{Monoid rings}

\begin{definition}
    A \textbf{ring} is an abelian group $(R,+,0)$ which is also a monoid $(R,\cdot,1)$ such that $\cdot$ distributes over $+$. We may also require $0\neq 1$.
\end{definition}
Some classical examples are: \begin{itemize}
    \item $\bZ$, the ring of integers;
    \item any field like $\bF_{p^n}$, $\bQ$, $\bR$, or $\bC$;
    \item $M_n(R)$ the ring of $n\times n$ matrices with entries in $R$, the first noncommutative example here;
    \item $R$-valued functions out of any set have a pointwise ring structure;
    \item the first Weyl algebra is roughly ``the ring of polynomial valued differential operators'', defined as \[
        A_1 := \bC[x,\partial] / (\partial x - x \partial = 1)
    \] you should think of this acting of $f\in \bC[x]$, generated by the product rule: \[
    \partial xf - x\partial f = f\frac{d}{dx}xf - x\frac{d}{dx}f = f.
    \]
\end{itemize}

\begin{definition}
    For a monoid $P$ and a ring $R$ the \textbf{monoid ring} is the set of finite $R$-linear sums:\[
    \sum_{p\in P}r_p p.
    \] The addition and multiplication come from the independent inclusions $R,P\subset R[P]$ by $r\mapsto r1_p$ and $p \mapsto 1_Rp$ respectively.
\end{definition}

Obviously, every group is a monoid, but here are some other examples:\begin{itemize}
    \item $\bN$ is a semiring, so a monoid under addition;
    \item given a set $X$, both $\cup$ and $\cap$ make $\cP(X)$ a monoid with $\emptyset,X$ the respective identities;
    \item the set of endomorphisms of an object in a category is always a monoid;
    \item let $C\subseteq \bR^n$ be a convex cone, i.e. $\bR_{\geq 0}C = C$ and $C+C = C$, then $P=C\cap\bZ^n$ is a monoid under addition, if $C=\abr{(1,0),(-1,2)}$ is generated by poitns in $\bZ$ we may not necessarily have $P$ generated as a monoid by these same elements (in this case $(0,1)\in P$ but cannot be realised as a $\bZ$-linear combination of $(1,0)$ and $(-1,2)$).
\end{itemize}

Most of the canonical examples of rings missed out earlier was because they can be realised as monoid rings: \begin{itemize}
    \item $R[x_1,\ldots,x_n]$ the ring of polynomials in $n$ variables, is just the monoid ring $R[\bN^n]$;
    \item $R[x_1,x_1^{-1},\ldots,x_n,x_n^{-1}]$ the ring of Laurent polynomials in $n$ varialbes, is the monoid ring $R[\bZ^n]$;
    \item we will see that a lot of ring homomorphisms are induced by monoid homomorphisms they are rings over, a first example of this is the isomorphism $\bC[\bZ/n\bZ]\cong\bC[x]/(x^n-1)$;
    \item we can define the quaternions $\bH$ as a quotient of $\bR[Q_8]$, where $Q_8$ is the quaternionic group $\{\pm 1,\pm i, \pm j,\pm k\}$, by the ideal $(1 + (-1), i + (-i),j+(-j),k+(-k))$;
    \item $\bR[P]$ from earlier is a subring of $\bR[x,y]$ which, as $P$ isn't generated by $\{(1,0),(-1,2)\}$, is actually $\bR[x,y,y^2/x]$.
\end{itemize}

\begin{definition}
    For two rings $S,R$ a \textbf{ring homomorphism} is a function $f:R\rightarrow S$ such that $f$ is a homomorphism of the additive groups and multiplicative monoids.
\end{definition}

Constructing ring is in general very hard, it is a lot of data.

\begin{definition}
    If $R$ is a ring and $f:P\rightarrow Q$ is a monoid homomorphism then there is an \textbf{induced homomorphism} $f_* : R[P]\rightarrow R[Q]$ given by: \[
    \sum_{p\in P}r_p p \mapsto \sum_{p\in P} r_pf(p).
    \]
\end{definition}

This makes a lot of interesting ring homomorphisms, anything more interesting belongs to the land of algebraic geometry, we will not discuss that here.

The \textbf{image} and \textbf{kernel} of a ring homomorphism are inhereted from the additive group homomorphism.

\begin{definition}
    An additive subgroup $I\leq R$ is a \textbf{left ideal} if $RI \subseteq I$. Right ideals and two-sided ideals are defined obviously.
\end{definition}

The kernel of a ring homomrophism is certainly a two-sided ideal. Conversely, to any ideal $I$ there is a unique ring structure on $R/I$ that makes $\varphi:R\rightarrow R/I$ a ring homomorphism.

\begin{definition}
    A \textbf{left unit} in $R$ is an element $u\in R$ such that there exists some $v\in R$ with $uv =1$.
\end{definition}

Are left units always right units? We know this to be true in commutative rings and $M_{n\times n}(k)$, but if we consider $GL(\bR^\bN)$, with the basis $\{e_1,e_2,\ldots\}$, then the linear map which sends each $e_i$ to $e_{i+1}$ is injective and has a left inverse, but is not surjective so has no right inverse. But, if $u$ has both a right inverse \textbf{and} a left inverse, then these will always be the same. The group of two-sided units is called $R^\times$.

\subsection{Classical algebra and whatnot}

You probably already know what an Euclidean domain, principal ideal domain, unique factorisation domain, and a Noetherian ring are.

There won't be a particularly rigorous discussion of the relationship, because I don't care. Look at any set of official notes for something of higher quality if you require it.

\begin{proposition}
    An Euclidean domain is a principal ideal domain.
    \begin{proof}
        Let $R$ be an Euclidean domain with degree function $\varphi$, and let $I\unlhd R$, consider $\varphi(I)$, this must have a smallest element, call it $a$. Now for any $i\in I$, we can write $i = qa + r$ with $\phi(r)<\phi(a)$, but $r$ must be in $I$ so cannot have degree less than that of $a$, so $r=0$. Therefore, $I=(a)$.
    \end{proof}
\end{proposition}

\begin{proposition}
    A ring is Noetherian iff all ideals are finitely generated.\begin{proof}
        Suppose an ideal $I\unlhd R$ does not admit a finite generating set. Find $i_0\in I$, and $i_1\in I\setminus(i_0)$, and recursively find $i_n\in I\setminus(i_0,i_1,\ldots,i_{n-1})$, this produces an ascending chain: \[
        (i_0) \subsetneq (i_0,i_1) \subsetneq (i_0,i_1,i_2) \subsetneq \cdots
        \] Conversely, suppose every ideal in $R$ is finitely generated, then for any ascending chain of ideals: \[
        I_0 \subset I_1 \subset I_2 \subset \cdots
        \] their union $I$ is also an ideal, which is finitely generated with its elements living in some $I_N$, thus the chain stabilises at this ideal.
    \end{proof}
\end{proposition}

\begin{proposition}
    A principal ideal domain is a unique factorisation domain.
    \begin{proof}
        First we show that any $r\in R$ a PID can't be an infinite product of irreducibles. Suppose such and $r$ exists, then as $r$ irreducible we can find a nontrivial factorisation $r=r_1s$, one of these must contain infinitely many irreducibles, let this be $r_1$. Repeat this process ad infinum to get a chain of ideals \[
        (r)\subsetneq (r_1) \subsetneq (r_2) \subsetneq \cdots
        \] as $R$ is a PID it is also Noetherian, so this ascending never stabilising chain of ideals cannot exist.\footnote{This proof is clearly nonconstructive as we are choosing contably many irreducible factorisations all at once}

        We now wish to show any such product of irreducibles \[
        r_1r_2\cdots r_n = r = s_1s_2\cdots s_m
        \] are equivalent. First note that in a PID an element is irreducible iff it is prime.\footnote{Given $r$ irreducible, if $r\mid ab$ we can consider $c=\gcd(r,a)$ and observe $c\mid r$. I'm only really interested in $R$ an ED, so lets assume that. We know $r=cd$ for some $d$, as $r$ is irreducible either $c$ is a unit, in which case we can write $xr+ya=c$ implying $xrb+yab=cb$, as $r=ab$ we have that $r\mid b$. If, instead, $c$ is an associate of $r$, then $r\mid a$. The other direction is easier.} We can now see each $r_i$ appears in the RHS, as we are living in an integral domain we can cancel and thus are done.
    \end{proof}
\end{proposition}

\subsection{Unique factorisation in polynomial rings}

\begin{definition}
    Let $R$ be a UFD, if $f\in R[x]$ the \textbf{content} of $f$ is the gcd of its nonzero coefficients, written $c(f)$.
\end{definition}

\begin{lemma}
    If $f,g\in R[x]$, then $c(fg)=c(f)c(g)$.\footnote{As with lots of things in this course, this must be taken up to multiplication by a unit.}
\end{lemma}

This really follows easily from

\begin{lemma}
    If $c(f)=c(g)=1$ then $c(fg)=1$.
    \begin{proof}
        Suppose $c(fg)\neq 1$, then there exists some prime $p$ dividing all the coefficients. First observe that as $R$ is an integral domain, so is $R[x]$. Now consider the projection mod $p$, as $c(f)=c(g)=1$ there exists no prime dividing all elements so they are both nonzero in $R/(p)[X]$ which we know to be an ID, but supposedly their product is $0$. This is clearly a contradiction.
    \end{proof}
\end{lemma}

For our next step we need the following 

\begin{theorem}[Gauss lemma]
    If $R$ is a UFD, and $f\in R[x]$ splits completely in $\Frac(R)[x]$, then $f$ splits completely in $R[x]$.
\end{theorem}

From these two statements you should apparently be able to deduce that if $R$ is a UFD, so is $R[X]$???

\subsection{Newton polytopes}

\subsection{Hilbert basis theorem}

\subsection{Invariant theory}

All rings are commutative.

\begin{definition}
    If $K$ is a field, a \textbf{$k$-algebra} is a ring $R$ continaing $k$. If $k\subset R_1,R_2$ are two such $k$-algebras, then a \textbf{$k$-algebra homomorphism} is a ring homomorphism which is the identity when restircted to $k$.
\end{definition}

$R=\bC[X_1,\ldots,X_n]$ is a $\bC$ algeabra, $\bZ$ is not a $k$-algebra for any $k$.

\begin{definition}
    A $k$-algebra $R$ is \textbf{finitely generated} if there exists a finite subset $S\{a_1,\ldots,a_n\}\subset R$ such $R$ is the smallest $k$-subalgebra containing $S$.

    Equivalently, if there exists a $k$-algebra homomorphism \[
    \ev_S:k[X_1,\ldots,X_n]\rightarrow R
    \] which send $f$ to $f(a_1,\ldots,a_n)$ and is surjective.
\end{definition}

So all f.g. $k$-algebras are rings of the form \[
k[X_1,\ldots,X_n]/I
\] for some ideal $I$ which, by the Hilbert basis theorem, will be finitely generated. We like these, because \textit{in principle} every questions about f.g. $k$-algebra (of which there are many of great intereset) can be answered algorithmically.

Even thought there are many interesting $k$-algebras which aren't finitely generated, like $\bC[[X]]$ the ring of power series in complex coefficients with radius of convergence $R>0$, so we can do pointwise operations.

The group $GL_n(\bC)$ has a \textbf{right}\footnote{People really don't like right actions, they like turning them into left actions by consider $G^\text{op}$ or explicitly acting  by inverses, we want to be able to efficiently compute things involving our actions, so we will tolerate just working with right actions.} action on $\bC[X_1,\ldots,X_n]$ by acting on $\bC^n$ as matrices before applying the polynomial.

\begin{definition}
    A \textbf{graded $k$-algebra} is a $k$-algebra \[
    R = \bigoplus_{n\in\bN}R_n
    \] such that $R_0=k$ and for each $n\geq 1$, $R_n$ is a finite dimension $k$-vector space, and for all $n,m\in\bN$, $R_nR_m\subset R_{n+m}$.

    A \textbf{graded $k$-algebra homomorphism} $f:R\rightarrow S$ should satisfy $f(R_n)\subseteq S_n$ for all $n\in \bN$.
\end{definition}

$k[X_1,\ldots,X_n]$ is the obvious example of a graded $k$-algebra by the degree of the polynomials, and the action of $GL_n(k)$ is in fact graded, as it preserves polynomial degree.

\section{Matrix Lie groups}

\end{document}